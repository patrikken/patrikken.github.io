[
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Getting Started with Compute Clusters\n\n\n\ncompute\n\ncluster\n\ntraining\n\n\n\nIn this post, I will walk you through the steps to get started with compute clusters, with a focus on Compute Canada.\n\n\n\n\n\nMay 22, 2025\n\n\nPatrik Kenfack\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/Getting Started with Compute Clusters/index.html",
    "href": "blog/posts/Getting Started with Compute Clusters/index.html",
    "title": "Getting Started with Compute Clusters",
    "section": "",
    "text": "ChatGPT generated image"
  },
  {
    "objectID": "blog/posts/Getting Started with Compute Clusters/index.html#digital-research-alliance-of-canada",
    "href": "blog/posts/Getting Started with Compute Clusters/index.html#digital-research-alliance-of-canada",
    "title": "Getting Started with Compute Clusters",
    "section": "Digital Research Alliance of Canada",
    "text": "Digital Research Alliance of Canada\nDigital Research Alliance of Canada is a national research infrastructure that provides high-performance computing resources to researchers across Canada. It is a partnership between the Canadian government and the private sector.\nIf you don‚Äôt have an account, you can sign up here. You will need to get sponsored by your supervisor to get your account approved."
  },
  {
    "objectID": "blog/posts/Getting Started with Compute Clusters/index.html#login-to-compute-canada",
    "href": "blog/posts/Getting Started with Compute Clusters/index.html#login-to-compute-canada",
    "title": "Getting Started with Compute Clusters",
    "section": "Login to Compute Canada",
    "text": "Login to Compute Canada\nOnce you have an account, you can login via ssh to the cluster. In case you don‚Äôt have ssh installed on your computer, you should follow the instructions here to install it.\nCompute Canada has several compute clusters located across Canada. Here is short list of those clusters:\n\nGraham\nCedar\nNarval\nBeluga\n\n\n\n\n\n\n\nCluster Names\n\n\n\nThe names of the clusters are chosen based some facts about the location of the cluster. For example, Beluga, located at √âcole de technologie sup√©rieure in Montreal, is named after the Beluga whale, which is found in the St.¬†Lawrence River.\n\n\nThe name of the cluster is used to login to a specific cluster. For example, to login to the Beluga cluster, you can use the following command:\nssh &lt;your_username&gt;@beluga.alliancecan.ca"
  },
  {
    "objectID": "blog/posts/Getting Started with Compute Clusters/index.html#login-nodes-vs-compute-nodes",
    "href": "blog/posts/Getting Started with Compute Clusters/index.html#login-nodes-vs-compute-nodes",
    "title": "Getting Started with Compute Clusters",
    "section": "Login nodes vs Compute nodes",
    "text": "Login nodes vs Compute nodes\nUnlike other cloud providers, login via ssh to a node does not give direct access to a compute machine. Instead, you will be on a login node from which you can submit jobs to the compute nodes.\n\nLogin nodes\nOnce you are logged in to the cluster, you will be on a login node. Login nodes are meant for interactive use and are not meant for running your large programs.\nThings you can do on the login nodes:\n\nGit clone your repository\nRun small interactive jobs (will be discussed below)\nEdit and submit jobs using a text editor\nView the status of your jobs\nTransfer files to and from the cluster\nCreate virtual environments and install packages\nSubmit jobs to the cluster (will be assigned to a compute node)\n\nThings you cannot do on the login nodes:\n\nRun large jobs\nUse a lot of memory\nUse a lot of CPU\n\n\n\n\nExample of ssh session on a login node. Login nodes generally have hostnames like beluga1, beluga2, etc.\n\n\n\n\nCompute nodes\nCompute nodes are meant for running large jobs. They have more memory and CPU than login nodes, and they have GPUs.\nWhen you submit a job, it is assigned to a compute node with the requested resources. We will get back to this below. Before that, let‚Äôs discuss file systems and where you should copy the files for your project."
  },
  {
    "objectID": "blog/posts/Getting Started with Compute Clusters/index.html#file-system",
    "href": "blog/posts/Getting Started with Compute Clusters/index.html#file-system",
    "title": "Getting Started with Compute Clusters",
    "section": "File System",
    "text": "File System\nOne of the most important thing to know about Compute Canada is the file system. There are three types of file systems with different properties and limitations: home, project, scratch\n Directory structure on Beluga\nThe home directory has small quota, a fixed number of filesystem objects and smaller storage space (generally up to 25 GB) and has dearly backups. The project directory is meant for long-term storage of data that is shared across users linked to a sponsor. The scratch directory is meant for temporary storage of data and has a larger fixed quota per user and a larger storage space (generally above to 2 PB of space).\n\n\n\n\n\n\nTip\n\n\n\nIf you are actively working on a project, I recommend that you copy your data (source code, datasets, etc.) to the scratch directory. This will save you time, and you don‚Äôt need to worry about the storage space. You can copy your data back to the project directory after your project is done or paused.\n\n\nNote that only the unused files on the scratch directory are deleted after 30 or 60 days, depending on the cluster. So if you are actively working on your project, you can use scratch and run everything as on your local machine, with logs files and results saved in your project directory.\nA better approach is to have your source code and data in the project directory and make sure your logs (e.g., training logs and models‚Äô checkpoints) and results are saved in the scratch directory. This way you can keep your project directory clean and you can have better control of your data. In another post, we will discuss how to speed up jobs using $SLURM_TMPDIR."
  },
  {
    "objectID": "blog/posts/Getting Started with Compute Clusters/index.html#job-submission",
    "href": "blog/posts/Getting Started with Compute Clusters/index.html#job-submission",
    "title": "Getting Started with Compute Clusters",
    "section": "Job Submission",
    "text": "Job Submission\nOnce you have your data in the scratch directory, you can submit a job to the cluster.\n\nModule loading\nBefore running any job, you need to load the modules for the software you need. For example, if you need to use Python 3.10, you need to load the Python module with the following command.\nmodule load python/3.10\nAfter loading the module, you can check if it is loaded correctly by running module list. Note that from here on, all the dependencies you install, with pip install for example, will be installed for this specific version of Python and will be available in your environment every time you load the module.\n\n\nVirtual environment\nAfter loading the module, you can also create a virtual environment for project-specific dependencies.\npython -m venv .venv\nsource .venv/bin/activate\n\n\nSubmitting a job\nAt this point, you should have your data in the scratch directory, installed the dependencies in your loaded module and created a virtual environment for project-specific dependencies.\nNow you can submit a job to the cluster.\nLet‚Äôs assume you have a Python script (my_script.py) that you want to run on the cluster, and you can pass some arguments to it, for example, the number of epochs and the dataset to use, etc. Normally, on your local machine, you run this script as follows:\npython my_script.py --num_epochs 60 --dataset mnist\nTo submit this job to the cluster, you need to create a job script that will be used to submit the job. A job script is a bash program that will load the necessary modules and specify the resources your Python program needs (number of nodes, number of cores, amount of memory, type and number of GPUs, etc.).\n\nJob script directives\nThe header of the job script contains directives to specify the resources your Python program needs. Here is an example of a job script header:\n#!/bin/bash\n#SBATCH --account=def-someuser\n#SBATCH --gpus-per-node=1         # Number of GPU(s) per node\n#SBATCH --node=1         # Number of GPU(s) per node\n#SBATCH --cpus-per-task=4         # CPU cores/threads\n#SBATCH --time=00:15:00           # Time limit in hh:mm:ss \n#SBATCH --mem=1024M\n\n#SBATCH --account=def-someuser is the account of your supervisor. To obtain what account you have access to, you can login to CCDB and click on My Account -&gt; My Resources and Allocations\n\n\n\n\nThe illustration above tells you what account you can use on a given cluster. For example, on Cedar, I can submit jobs using #SBATCH --account=def-ebrahimi.\n\n\n\n#SBATCH --gpus-per-node=1 is the number of GPUs per node, in general, you set up 4 GPUs per node. And #SBATCH --node=1 specifies the number of nodes you want to use.\nIf you want to use 2 GPUs per node, you can set #SBATCH --gpus-per-node=2 and #SBATCH --node=1\nIf you want to use 2 nodes with 4 GPUs per node, you can set #SBATCH --gpus-per-node=4 and #SBATCH --node=2, i.e., your job will be using 8 GPUs in total.\n#SBATCH --cpus-per-task=4 is the number of CPU cores per task. This is the number of CPU cores your program will use.\n#SBATCH --time=00:15:00 is the time limit for your job. This is the maximum time your job can run. In general, you don‚Äôt know exactly how long your job will run, so you should set a time limit that is long enough to run your job and adjust it later.\n#SBATCH --mem=1024M is the amount of memory your job will use. This is the maximum amount of memory your job can use. You can set it in GB as --mem=32G you request 32 GB of memory. If you are not sure how much memory your job will use, you can set it to a large value and adjust it later."
  },
  {
    "objectID": "blog/posts/Getting Started with Compute Clusters/index.html#putting-it-all-together",
    "href": "blog/posts/Getting Started with Compute Clusters/index.html#putting-it-all-together",
    "title": "Getting Started with Compute Clusters",
    "section": "Putting it all together",
    "text": "Putting it all together\nHere is an example of a job script that you can use to submit your job:\n#!/bin/bash\n#SBATCH --account=def-ebrahimi\n#SBATCH --gpus-per-node=1         # Number of GPU(s) per node\n#SBATCH --node=1         # Number of GPU(s) per node\n#SBATCH --cpus-per-task=4         # CPU cores/threads\n#SBATCH --time=00:15:00           # Time limit in hh:mm:ss \n#SBATCH --mem=1024M\n\n\nmodule load python/3.10 # Load the Python module version\nsource .venv/bin/activate # Activate the virtual environment\n\n\npython my_script.py --num_epochs 60 --dataset mnist\nFinally, you can submit your job by running the following command:\nsbatch my_script.sh # You will see a job ID in the output.\nDepending on the cluster and the resources you requested, your job will be assigned to a compute node and will start running. A log file, named slurm-&lt;job_id&gt;.out, will be created in the main directory of your project.\nYou can check the status of your jobs by running the following command:\nsqueue -u &lt;your_username&gt;\nIn case you want to cancel your job, you can run the following command:\nscancel &lt;job_id&gt;\nIf you want to cancel all your jobs, you can run the following command:\nscancel -u &lt;your_username&gt;"
  },
  {
    "objectID": "blog/posts/Getting Started with Compute Clusters/index.html#interactive-jobs",
    "href": "blog/posts/Getting Started with Compute Clusters/index.html#interactive-jobs",
    "title": "Getting Started with Compute Clusters",
    "section": "Interactive jobs",
    "text": "Interactive jobs\nSometimes you want to run a job interactively, for example, to debug your code or to run a small test. You can do this by running the following command:\nsalloc --time=00:30:00 --account=def-ebrahimi --mem=48000 --gpus-per-node=4 --cpus-per-task=1\nSimilarly to job scripts, parameters of the salloc command specify the resources you need for your interactive job session.\nThe illustration below shows an interactive job session on the Beluga cluster.\n Interactive job session on the Beluga cluster\nYou can see that salloc moved us from a login node (beluga3) to a compute node (bg12103). This compute node has requested resources, i.e., 4 GPUs, 1 CPU core, 48 GB of memory and 30 minutes of time. However, before running your Python program interactively, you need to type commands to load the module and activate the virtual environment.\n\n\n\n\n\n\nInteractive jobs\n\n\n\nThe more you request resources (time, GPUs, CPU cores, memory), the longer you will wait to get an interactive compute node. You can use them to test your code before submitting a job script.\n\n\nIn the next post, we will discuss how to speed up jobs using $SLURM_TMPDIR."
  },
  {
    "objectID": "blog/posts/Getting Started with Compute Clusters/index.html#conclusion",
    "href": "blog/posts/Getting Started with Compute Clusters/index.html#conclusion",
    "title": "Getting Started with Compute Clusters",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we have covered the basics of compute clusters, including how to log in to the cluster, how to submit jobs, and how to use interactive jobs. I believe that from here on, you can navigate the documentation of the compute cluster to learn more options and best practices when using the cluster.\nFor more training material, you can check these beginner‚Äôs guides covering Linux commands, git & version control, and Python programming."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "blog/posts/In-Context-Learning/index.html",
    "href": "blog/posts/In-Context-Learning/index.html",
    "title": "In-Context Learning with Tabular Data.",
    "section": "",
    "text": "Tabular data is a modality commonly found in various fields including finance, healthcare, and social sciences. They are structured data represented in a row and column format. In machine learning, row are seen as data points and column as features. The features can be numerical, categorical, or a mix of both.\nIn the classical classification machine learning pipeline, the training step invole updating the model parameter to minimize the loss function. In-Context Learning (ICL) on the other hand, a new learning paradigm where the model can make predictions using few examples as context and no model update is required.\nIn this post, I will introduce in-context learning with tabular foundation models and how to use it to improve the performance of your models."
  },
  {
    "objectID": "blog/posts/In-Context-Learning/index.html#what-is-in-context-learning",
    "href": "blog/posts/In-Context-Learning/index.html#what-is-in-context-learning",
    "title": "In-Context Learning with Tabular Data.",
    "section": "",
    "text": "ICL is new paradigm that have emerged as a capability of large language models (LLMs) to make predictions using few examples as context and no model update is required Brown, Tom, et al.."
  },
  {
    "objectID": "blog/posts/In-Context-Learning/index.html#tabular-foundation-models",
    "href": "blog/posts/In-Context-Learning/index.html#tabular-foundation-models",
    "title": "In-Context Learning with Tabular Data.",
    "section": "Tabular Foundation Models",
    "text": "Tabular Foundation Models\nBefore\n!pip install tabpfn tabicl scikit-learn"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Patrik Kenfack",
    "section": "",
    "text": "I‚Äôm a PhD Candidate at √âTS Montr√©al - Mila advised by Ulrich A√Øvodji and Samira Ebrahimi Kahou. My thesis investigates bias mitigation in machine learning under various constraints on sensitive information. Before joining √âTS Montr√©al, I was a research assistant at Machine Learning and Knowledge Representation Lab at Innopolis University, advised by Adil Khan. My work focused on studying fairness aspect of machine learning methods such as ensemble learning, GANs, continual learning, and representation learning.\nI graduated with a Master‚Äôs Degree in Computer Science and a Bachelor‚Äôs in Mathematics and CS at University of Dschang.\nResearch Interest:\n\nApplied Machine Learning\nAlgorithmic Fairness\nPrivacy-Preserving Machine Learning (PPML)\nResponsible AI\nFoundation Models for Structured Data\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nDescription\n\n\n\n\nFev 2026\nüõ†Ô∏è Service\nReviewing for FAcct.\n\n\nJan 2026\nüìÑ Paper\nOur paper Towards Fair In-Context Learning with Tabular Foundation Models is accepted at TMLR\n\n\nJan 2026\nüõ†Ô∏è Service\nReviewing for CVPR\n\n\nNov 2025\nüìÑ Paper\nOur paper Adaptive Group Robust Ensemble Knowledge Distillation is accepted at TMLR\n\n\nJune 2025\nüìÑ Paper\nOur paper Towards Fair In-Context Learning with Tabular Foundation Models is accepted at ICML 2025 FMSD workshop\n\n\nMay 2025\nüìÑ Paper\nOur recent paper Towards Fair In-Context Learning with Tabular Foundation Models is out!\n\n\nMar 2025\nüèÜ Award\nSecured second place for the 3-Minute Research Presentation Contest at the 2025 ETS student researcher conference\n\n\nMar 2025\nüìÑ Paper\nTwo papers accepted at SCSL @ ICLR 2025. 1. GradTune: Last-layer Fine-tuning for Group Robustness Without Group Annotation. 2. Towards personalized healthcare without harm via bias modulation\n\n\nFeb 2025\nüõ†Ô∏è Service\nReviewing for FAcct.\n\n\nOct 2024\nüìÑ Paper\nOur paper Adaptive Group Robust Ensemble Knowledge Distillation was accepted at Neurips2024 AFME workshop.\n\n\nAug 2024\nüìÑ Paper\nOur paper Fairness Under Demographic Scarce Regime was accepted at TMLR.\n\n\nAug 2024\nüõ†Ô∏è Service\nInvited to serve as a reviewer at ICLR 2025.\n\n\nJul 2024\nüõ†Ô∏è Service\nInvited to provide an emergency review at Neurips 2024.\n\n\nJun 2024\nüìÑ Paper\nOur Survey on Fairness Without Demographics was accepted at TMLR.\n\n\nApr 2024\nüõ†Ô∏è Service\nSelected to join the Mila EDI committee.\n\n\nJan 2024\nüõ†Ô∏è Service\nReviewer for FAcct.\n\n\nAug 2023\nüèÜ Award\nOur TISL lab team won a $10,000 prize at the 2023 Kaggle AI Report Competition. Our report, Exploring the Landscape of AI Ethics, secured first place in the AI Ethics category.\n\n\nMar 2023\nüõ†Ô∏è Service\nReviewer for ICCV 2023.\n\n\nApr 2023\nüèÜ Award\nAwarded the Excellence Scholarships ‚Äì EDI in Research from Mila, supporting equity, diversity, and inclusion in AI research.\n\n\nFeb 2023\nüõ†Ô∏è Service\nReviewer for FAccT 2023.\n\n\nJan 2023\nüé§ Talk\nPresented our work Learning Fair Representations Through Uniformly Distributed Sensitive Attributes at SaTML 2023.\n\n\n\n\n\n\n\n \n  \n   \n  \n    \n     LinkedIn\n  \n  \n    \n     Github\n  \n  \n      Email\n  \n  \n      Google Scholar"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Patrik Kenfack",
    "section": "",
    "text": "I‚Äôm a PhD Candidate at √âTS Montr√©al - Mila advised by Ulrich A√Øvodji and Samira Ebrahimi Kahou. My thesis investigates bias mitigation in machine learning under various constraints on sensitive information. Before joining √âTS Montr√©al, I was a research assistant at Machine Learning and Knowledge Representation Lab at Innopolis University, advised by Adil Khan. My work focused on studying fairness aspect of machine learning methods such as ensemble learning, GANs, continual learning, and representation learning.\nI graduated with a Master‚Äôs Degree in Computer Science and a Bachelor‚Äôs in Mathematics and CS at University of Dschang.\nResearch Interest:\n\nApplied Machine Learning\nAlgorithmic Fairness\nPrivacy-Preserving Machine Learning (PPML)\nResponsible AI\nFoundation Models for Structured Data"
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Patrik Kenfack",
    "section": "",
    "text": "Date\nType\nDescription\n\n\n\n\nFev 2026\nüõ†Ô∏è Service\nReviewing for FAcct.\n\n\nJan 2026\nüìÑ Paper\nOur paper Towards Fair In-Context Learning with Tabular Foundation Models is accepted at TMLR\n\n\nJan 2026\nüõ†Ô∏è Service\nReviewing for CVPR\n\n\nNov 2025\nüìÑ Paper\nOur paper Adaptive Group Robust Ensemble Knowledge Distillation is accepted at TMLR\n\n\nJune 2025\nüìÑ Paper\nOur paper Towards Fair In-Context Learning with Tabular Foundation Models is accepted at ICML 2025 FMSD workshop\n\n\nMay 2025\nüìÑ Paper\nOur recent paper Towards Fair In-Context Learning with Tabular Foundation Models is out!\n\n\nMar 2025\nüèÜ Award\nSecured second place for the 3-Minute Research Presentation Contest at the 2025 ETS student researcher conference\n\n\nMar 2025\nüìÑ Paper\nTwo papers accepted at SCSL @ ICLR 2025. 1. GradTune: Last-layer Fine-tuning for Group Robustness Without Group Annotation. 2. Towards personalized healthcare without harm via bias modulation\n\n\nFeb 2025\nüõ†Ô∏è Service\nReviewing for FAcct.\n\n\nOct 2024\nüìÑ Paper\nOur paper Adaptive Group Robust Ensemble Knowledge Distillation was accepted at Neurips2024 AFME workshop.\n\n\nAug 2024\nüìÑ Paper\nOur paper Fairness Under Demographic Scarce Regime was accepted at TMLR.\n\n\nAug 2024\nüõ†Ô∏è Service\nInvited to serve as a reviewer at ICLR 2025.\n\n\nJul 2024\nüõ†Ô∏è Service\nInvited to provide an emergency review at Neurips 2024.\n\n\nJun 2024\nüìÑ Paper\nOur Survey on Fairness Without Demographics was accepted at TMLR.\n\n\nApr 2024\nüõ†Ô∏è Service\nSelected to join the Mila EDI committee.\n\n\nJan 2024\nüõ†Ô∏è Service\nReviewer for FAcct.\n\n\nAug 2023\nüèÜ Award\nOur TISL lab team won a $10,000 prize at the 2023 Kaggle AI Report Competition. Our report, Exploring the Landscape of AI Ethics, secured first place in the AI Ethics category.\n\n\nMar 2023\nüõ†Ô∏è Service\nReviewer for ICCV 2023.\n\n\nApr 2023\nüèÜ Award\nAwarded the Excellence Scholarships ‚Äì EDI in Research from Mila, supporting equity, diversity, and inclusion in AI research.\n\n\nFeb 2023\nüõ†Ô∏è Service\nReviewer for FAccT 2023.\n\n\nJan 2023\nüé§ Talk\nPresented our work Learning Fair Representations Through Uniformly Distributed Sensitive Attributes at SaTML 2023."
  },
  {
    "objectID": "instance/news.html",
    "href": "instance/news.html",
    "title": "Patrik Kenfack",
    "section": "",
    "text": "May 2025\nüìÑ Paper\nOur recent paper Towards Fair In-Context Learning with Tabular Foundation Models is out!\n\n\nMar 2025\nüèÜ Award\nSecured second place for the 3-Minute Research Presentation Contest at the 2025 ETS student researcher conference\n\n\nMar 2025\nüìÑ Paper\nTwo papers accepted at SCSL @ ICLR 2025.  1. GradTune: Last-layer Fine-tuning for Group Robustness Without Group Annotation.  2. Towards personalized healthcare without harm via bias modulation\n\n\nFev 2025\nüõ†Ô∏è Service\nReviewing for FAcct.\n\n\nOct 2024\nüìÑ Paper\nOur paper Adaptive Group Robust Ensemble Knowledge Distillation was accepted at Neurips2024 AFME workshop.\n\n\nAug 2024\nüìÑ Paper\nOur paper Fairness Under Demographic Scarce Regime was accepted at TMLR.\n\n\n\nüõ†Ô∏è Service\nInvited to serve as a reviewer at ICLR 2025.\n\n\nJul 2024\nüõ†Ô∏è Service\nInvited to provide an emergency review at Neurips 2024.\n\n\nJun 2024\nüìÑ Paper\nOur Survey on Fairness Without Demographics was accepted at TMLR.\n\n\nApr 2024\nüõ†Ô∏è Service\nSelected to join the Mila EDI committee.\n\n\nJan 2024\nüõ†Ô∏è Service\nReviewer for FAcct.\n\n\nAug 2023\nüèÜ Award\nOur TISL lab team won a $10,000 prize at the 2023 Kaggle AI Report Competition. Our report, Exploring the Landscape of AI Ethics, secured first place in the AI Ethics category.\n\n\nMar 2023\nüõ†Ô∏è Service\nReviewer for ICCV 2023.\n\n\nApr 2023\nüèÜ Award\nAwarded the Excellence Scholarships ‚Äì EDI in Research from Mila, supporting equity, diversity, and inclusion in AI research.\n\n\nFeb 2023\nüõ†Ô∏è Service\nReviewer for FAccT 2023.\n\n\nJan 2023\nüé§ Talk\nPresented our work Learning Fair Representations Through Uniformly Distributed Sensitive Attributes at SaTML 2023."
  },
  {
    "objectID": "instance/In-Context-Learning/index.html",
    "href": "instance/In-Context-Learning/index.html",
    "title": "In-Context Learning with Tabular Data.",
    "section": "",
    "text": "Tabular data is a modality commonly found in various fields including finance, healthcare, and social sciences. They are structured data represented in a row and column format. In machine learning, row are seen as data points and column as features. The features can be numerical, categorical, or a mix of both.\nIn the classical classification machine learning pipeline, the training step invole updating the model parameter to minimize the loss function. In-Context Learning (ICL) on the other hand, a new learning paradigm where the model can make predictions using few examples as context and no model update is required.\nIn this post, I will introduce in-context learning with tabular foundation models and how to use it to improve the performance of your models."
  },
  {
    "objectID": "instance/In-Context-Learning/index.html#tabular-foundation-models",
    "href": "instance/In-Context-Learning/index.html#tabular-foundation-models",
    "title": "In-Context Learning with Tabular Data.",
    "section": "Tabular Foundation Models",
    "text": "Tabular Foundation Models\nBefore\n!pip install tabpfn tabicl scikit-learn"
  }
]